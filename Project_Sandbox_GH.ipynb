{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import twitter\n",
    "import simplejson as json\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "consumer_key = ''\n",
    "consumer_secret = ''\n",
    "access_token = ''\n",
    "access_token_secret = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "oauth = twitter.OAuth(access_token, access_token_secret, consumer_key, consumer_secret)\n",
    "t = twitter.Twitter(auth=oauth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Query the tweets - the result is a 'twitter.api.TwitterDictResponse' object \n",
    "tweets_data = t.search.tweets(q='boogie cousins', result_type='recent', lang='en', count=100)\n",
    "# Convert the TwitterDictResponse object to a json file \n",
    "tweets_json = json.dumps(tweets_data)\n",
    "# Convert json file to a python dictionary \n",
    "tweets = json.loads(tweets_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets with repetition is 100.\n",
      "Number of tweets without repetition is 73.\n"
     ]
    }
   ],
   "source": [
    "# Place all tweets into a list \n",
    "tweets_list = []\n",
    "for i in range(100):\n",
    "    tweets_list.append(tweets['statuses'][i]['text'])\n",
    "print('Number of tweets with repetition is ', len(tweets_list), '.', sep = '')\n",
    "# To avoid repetition from retweets, remove duplicates \n",
    "tweets_list = list(set(tweets_list))\n",
    "print('Number of tweets without repetition is ', len(tweets_list), '.', sep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Popular Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Query the tweets - the result is a 'twitter.api.TwitterDictResponse' object \n",
    "poptweets_data = t.search.tweets(q='boogie cousins', result_type='popular', lang='en', count=100)\n",
    "# Convert the TwitterDictResponse object to a json file \n",
    "poptweets_json = json.dumps(poptweets_data)\n",
    "# Convert json file to a python dictionary \n",
    "poptweets = json.loads(poptweets_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Place all tweets into a list \n",
    "poptweets_list = []\n",
    "for i in range(14):\n",
    "    poptweets_list.append(poptweets['statuses'][i]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Necessary Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "\n",
    "from nltk.classify import SklearnClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "from nltk.corpus import twitter_samples\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "import re\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Train and Test for Naive Bayes Classifier \n",
    "\n",
    "Steps performed in this section: \n",
    "1. Create a function that puts words into format that the classifier understands. \n",
    "2. Load positive and negative tweets into respective lists, run them through step 1 function. \n",
    "3. Make train and test set. \n",
    "4. Train classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'hopeless for tmr :(',\n",
       " u\"Everything in the kids section of IKEA is so cute. Shame I'm nearly 19 in 2 months :(\",\n",
       " u'@Hegelbon That heart sliding into the waste basket. :(',\n",
       " u'\\u201c@ketchBurning: I hate Japanese call him \"bani\" :( :(\\u201d\\n\\nMe too',\n",
       " u'Dang starting next week I have \"work\" :(',\n",
       " u\"oh god, my babies' faces :( https://t.co/9fcwGvaki0\",\n",
       " u'@RileyMcDonough make me smile :((',\n",
       " u'@f0ggstar @stuartthull work neighbour on motors. Asked why and he said hates the updates on search :( http://t.co/XvmTUikWln',\n",
       " u'why?:(\"@tahuodyy: sialan:( https://t.co/Hv1i0xcrL2\"',\n",
       " u'Athabasca glacier was there in #1948 :-( #athabasca #glacier #jasper #jaspernationalpark #alberta #explorealberta #\\u2026 http://t.co/dZZdqmf7Cz']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = twitter_samples.strings('negative_tweets.json')\n",
    "tweets[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'good': True, 'g': True, '&': True, 'I': True, 'never': True, 'idea': True, 'going': True, 'meet': True, 'amp': True, ';': True, \"'m\": True, 'really': True}, 'negative')\n"
     ]
    }
   ],
   "source": [
    "# Unsophisticated, piecewise version \n",
    "def format_words(words):\n",
    "    useful_words = [word for word in words if word not in stopwords.words(\"english\")]\n",
    "    my_dict = dict([(word, True) for word in useful_words])\n",
    "    return(my_dict)\n",
    "\n",
    "tweets = twitter_samples.strings('negative_tweets.json')\n",
    "for i in range(len(tweets)):\n",
    "        tweets[i] = tweets[i].replace(':', \"\").replace(')', '').replace('(', '').replace('\\\\', '').replace('/', '')\n",
    "        tweets[i] = tweets[i].encode('ascii', 'ignore')\n",
    "        tweets[i] = word_tokenize(tweets[i])\n",
    "\n",
    "neg_tweets = []\n",
    "[neg_tweets.append((format_words(tweet), 'negative')) for tweet in tweets]\n",
    "print(neg_tweets[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Create Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def format_words(tweets, tweet_type):\n",
    "    # TWEET_TYPE MUST BE EITHER positive OR negative\n",
    "    \n",
    "    # First, clean the tweets (replace smileys, change format to string, tokenize words)\n",
    "    for i in range(len(tweets)):\n",
    "        tweets[i] = tweets[i].replace(':', \"\").replace(')', '').replace('(', '').replace('\\\\', '').replace('/', '')\n",
    "        tweets[i] = tweets[i].encode('ascii', 'ignore')\n",
    "        tweets[i] = word_tokenize(tweets[i])\n",
    "    \n",
    "    # Create a list that will contain the dictionary of words in each tweet\n",
    "    words = []\n",
    "    for tweet in tweets:\n",
    "        # Remove stopwords\n",
    "        useful_words = [word for word in tweet if word not in stopwords.words('english')]\n",
    "        # Create dictionary\n",
    "        my_dict = dict([(word, True) for word in useful_words])\n",
    "        # Append to words\n",
    "        words.append((my_dict, tweet_type))\n",
    "    # Return final list\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Load and clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "negative = twitter_samples.strings('negative_tweets.json')\n",
    "neg_tweets = format_words(negative, 'negative')\n",
    "positive = twitter_samples.strings('positive_tweets.json')\n",
    "pos_tweets = format_words(positive, 'positive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Create train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(neg_tweets[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6700, 3300)\n"
     ]
    }
   ],
   "source": [
    "train = neg_tweets[1650:] + pos_tweets[1650:]\n",
    "test = neg_tweets[:1650] + pos_tweets[:1650]\n",
    "print(len(train),  len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Train classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75.303030303\n"
     ]
    }
   ],
   "source": [
    "classifier = NaiveBayesClassifier.train(train)\n",
    "accuracy = nltk.classify.util.accuracy(classifier, test)\n",
    "print(accuracy * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Classifying an Example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I liked a @YouTube video https://t.co/eyE7RQWVMM Boogie Cousins to Golden State...WHAT?! | Strong Arm Sports Podcast Ep183\n",
      "{'liked': True, '...': True, 'Cousins': True, 'Boogie': True, 'Sports': True, 'State': True, 'video': True, '!': True, 'WHAT': True, 'YouTube': True, 'to': True, '//t.co/eyE7RQWVMM': True, 'https': True, 'Strong': True, '?': True, '@': True, 'I': True, 'Podcast': True, 'a': True, 'Golden': True, 'Ep183': True, ':': True, '|': True, 'Arm': True}\n",
      "\n",
      " positive\n"
     ]
    }
   ],
   "source": [
    "example = tweets_list[4]\n",
    "type(example)\n",
    "print(example)\n",
    "words = word_tokenize(example)\n",
    "useful_words = [word for word in words if word not in stopwords.words('english')]\n",
    "exp = dict([(word, True) for word in words])\n",
    "print(exp)\n",
    "print('\\n', classifier.classify(exp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above example is correctly identified as positive - Kevin Durant is welcoming Boogie. A very wholesome sentiment, indeed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Classifier\n",
    "\n",
    "Improvements to be made: \n",
    "\n",
    "1. Remove mentions of usernames (@LebronJames)\n",
    "\n",
    "2. Remove punctuation. \n",
    "\n",
    "3. Remove retweet (RT). \n",
    "\n",
    "4. Remove urls. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def format_words(tweets_input, tweet_type):\n",
    "    # TWEET_TYPE MUST BE EITHER positive OR negative\n",
    "    \n",
    "    tweets = deepcopy(tweets_input)\n",
    "    \n",
    "    for i in range(len(tweets)):\n",
    "        # Remove smileys\n",
    "        tweets[i] = tweets[i].replace(':', \"\").replace(')', '').replace('(', '').replace('\\\\', '').replace('/', '')\n",
    "        # Change format to strings\n",
    "        tweets[i] = tweets[i].encode('ascii', 'ignore')\n",
    "        # Remove usernames - Not helpful\n",
    "        #tweets[i] = re.sub(r'@\\S+', '', tweets[i])\n",
    "        # Remove urls - Not helpful\n",
    "        tweets[i] = re.sub(r'http\\S+', '', tweets[i])\n",
    "        # Remove puncutation\n",
    "        tweets[i] = re.sub(r'[^\\w\\s]', '', tweets[i])\n",
    "        # Remove indication of retweet (RT)\n",
    "        tweets[i] = re.sub(r'RT\\s', '', tweets[i])\n",
    "        # Tokenize words\n",
    "        tweets[i] = word_tokenize(tweets[i])\n",
    "    \n",
    "    # Create a list that will contain the dictionary of words in each tweet\n",
    "    words = []\n",
    "    for tweet in tweets:\n",
    "        # Remove stopwords\n",
    "        useful_words = [word for word in tweet if word not in stopwords.words('english')]\n",
    "        # Create dictionary\n",
    "        my_dict = dict([(word, True) for word in useful_words])\n",
    "        # Append to words\n",
    "        words.append((my_dict, tweet_type))\n",
    "    # Return final list\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "negative = twitter_samples.strings('negative_tweets.json')\n",
    "neg_tweets = format_words(negative, 'negative')\n",
    "positive = twitter_samples.strings('positive_tweets.json')\n",
    "pos_tweets = format_words(positive, 'positive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6700 3300\n",
      "76.5757575758\n"
     ]
    }
   ],
   "source": [
    "train = neg_tweets[1650:] + pos_tweets[1650:]\n",
    "test = neg_tweets[:1650] + pos_tweets[:1650]\n",
    "print(len(train),  len(test))\n",
    "classifier = NaiveBayesClassifier.train(train)\n",
    "accuracy = nltk.classify.util.accuracy(classifier, test)\n",
    "print(accuracy * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Baseline - 75.303\n",
    " - Removing urls and usernames reduce accuracy - 75.2. \n",
    " - Just removing punctuation improves - 76.57. \n",
    " - Doing all three - 76.18\n",
    " - punctuation and usernames - 76.18\n",
    " - punctuation and urls - 76.57\n",
    "\n",
    "Conclusion: Remove urls and punctuation. \n",
    "\n",
    " - punctuation, urls, removal of RT - 76.57. \n",
    "\n",
    "Might as well remove RT since it'll be heavy in final data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a Support Vector Classifier and Bernoulli Naive Bayes don't improve the accuracy - observe below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy: 0.765757575758\n",
      "Bernoulli NB: 0.765757575758\n"
     ]
    }
   ],
   "source": [
    "classif = SklearnClassifier(SVC()).train(train)\n",
    "print('SVM Accuracy:', nltk.classify.util.accuracy(classifier, test))\n",
    "\n",
    "classif = SklearnClassifier(BernoulliNB()).train(train)\n",
    "print('Bernoulli NB:', nltk.classify.util.accuracy(classifier, test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Our Tweets\n",
    "\n",
    "First, we need to redefine how our format_words function works since we don't want to give it a class. That was only for our training and testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def format_words_(tweets_input):\n",
    "    \n",
    "    # Create a copy of input list \n",
    "    tweets = deepcopy(tweets_input)\n",
    "    \n",
    "    for i in range(len(tweets)):\n",
    "        # Remove smileys\n",
    "        tweets[i] = tweets[i].replace(':', \"\").replace(')', '').replace('(', '').replace('\\\\', '').replace('/', '')\n",
    "        # Change format to strings\n",
    "        tweets[i] = tweets[i].encode('ascii', 'ignore')\n",
    "        # Remove usernames - Not helpful\n",
    "        #tweets[i] = re.sub(r'@\\S+', '', tweets[i])\n",
    "        # Remove urls - Not helpful\n",
    "        tweets[i] = re.sub(r'http\\S+', '', tweets[i])\n",
    "        # Remove puncutation\n",
    "        tweets[i] = re.sub(r'[^\\w\\s]', '', tweets[i])\n",
    "        # Remove indication of retweet (RT)\n",
    "        tweets[i] = re.sub(r'RT\\s', '', tweets[i])\n",
    "        # Tokenize words\n",
    "        tweets[i] = word_tokenize(tweets[i])\n",
    "    \n",
    "    # Create a list that will contain the dictionary of words in each tweet\n",
    "    words = []\n",
    "    for tweet in tweets:\n",
    "        # Remove stopwords\n",
    "        useful_words = [word for word in tweet if word not in stopwords.words('english')]\n",
    "        # Create dictionary\n",
    "        my_dict = dict([(word, True) for word in useful_words])\n",
    "        # Append to words\n",
    "        words.append(my_dict)\n",
    "    # Return final list\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_data = format_words_(tweets_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos = 0\n",
    "neg = 0\n",
    "for tweet in input_data:\n",
    "    result = classifier.classify(tweet)\n",
    "    if result == 'negative':\n",
    "        neg = neg + 1\n",
    "    if result == 'positive':\n",
    "        pos = pos + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "About 23.29% of tweets regarding Boogie Cousins were negative.\n"
     ]
    }
   ],
   "source": [
    "print('About {}% of tweets regarding Boogie Cousins were negative.'.format(round((float(neg)/(neg+pos)*100), 2)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
