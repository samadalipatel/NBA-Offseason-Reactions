{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import praw\n",
    "from praw.models import MoreComments\n",
    "import simplejson as json\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import string as s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Reddit/Subreddit Instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(client_id='', \n",
    "                     client_secret='', \n",
    "                     user_agent='', \n",
    "                     username='', \n",
    "                     password='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LeBron Joining the Lakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the thread we're using \n",
    "submission = reddit.submission(url='https://www.reddit.com/r/nba/comments/8ve9rs/withers_lebron_signing_with_lakers/')\n",
    "# Create empty list \n",
    "lebron = []\n",
    "# Append top-level comments to list \n",
    "for top_level_comment in submission.comments:\n",
    "    if isinstance(top_level_comment, MoreComments):\n",
    "        continue\n",
    "    lebron.append(top_level_comment.body)\n",
    "# The top comment is a stickied comment with no score, so remove it \n",
    "lebron = lebron[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "as a lakers fan since july 2018 i can't put into words how much this means to me\n",
      "\n",
      "\n",
      "Comments moving so fast no one will know I love my wife \n",
      "\n",
      "\n",
      "Does this hurt his chances of going to Houston?\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(lebron[i])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'e1motzb', u'e1mpdfi', u'e1mosv8', u'e1moxl5', u'e1mojxq']\n"
     ]
    }
   ],
   "source": [
    "# Determining number of upvotes\n",
    "ids = []\n",
    "for top_level_comment in submission.comments:\n",
    "    if isinstance(top_level_comment, MoreComments):\n",
    "        continue\n",
    "    ids.append(top_level_comment.id)\n",
    "print(ids[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "votes = []\n",
    "for id_no in ids[1:10]: \n",
    "    comment = reddit.comment(id = id_no)\n",
    "    votes.append(comment.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113407"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(votes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tasks for Cleaning: \n",
    "    \n",
    "1. Remove newlines. \n",
    "    \n",
    "2. Remove urls. \n",
    "\n",
    "3. Make all words lowercase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Model\n",
    "\n",
    "First, we'll train the model on the opinion-lexicon library. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Necessary Imports and Formatting Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "\n",
    "from nltk.classify import SklearnClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "from nltk.corpus import opinion_lexicon\n",
    "from nltk.corpus import subjectivity\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_words = list(opinion_lexicon.positive())\n",
    "pos_words = [word.encode('ascii', 'ignore') for word in pos_words]\n",
    "positive = []\n",
    "for word in pos_words:\n",
    "    positive.append(tuple([dict([[word, True]]), 'positive' ]))\n",
    "    \n",
    "neg_words = list(opinion_lexicon.negative())\n",
    "neg_words = [word.encode('ascii', 'ignore') for word in neg_words]\n",
    "negative = []\n",
    "for word in neg_words:\n",
    "    negative.append(tuple([dict([[word, True]]), 'negative']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following will determine how to train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "602.0\n",
      "1435.0\n"
     ]
    }
   ],
   "source": [
    "print(round(len(positive)*.30))\n",
    "print(round(len(negative)*.30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3205"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(negative[1578:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = positive[661:] + negative[1578:]\n",
    "test = positive[:661] + negative[:1578]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70.47789191603394\n"
     ]
    }
   ],
   "source": [
    "classifier = NaiveBayesClassifier.train(train)\n",
    "accuracy = nltk.classify.util.accuracy(classifier, test)\n",
    "print(accuracy * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mistake! I didn't split the data properly. Keep in mind for the final report. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing the Model\n",
    "\n",
    "We still need to clean the data. We'll place all the steps within the format_words function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check if output would have been the same with a function \n",
    "def format_words(comments_input):\n",
    "    # Create a copy of input list \n",
    "    new_comments = deepcopy(comments_input)\n",
    "    \n",
    "    for i in range(len(new_comments)): \n",
    "        # First, change coding from unicode to string \n",
    "        new_comments[i] = new_comments[i].encode('ascii', 'ignore')\n",
    "        # Remove newlines, punctuation, and urls \n",
    "        new_comments[i] = re.sub(r'\\n', ' ', new_comments[i])\n",
    "        new_comments[i] = re.sub(r'[^\\w\\s]', '', new_comments[i])\n",
    "        new_comments[i] = re.sub(r'http\\S+', '', new_comments[i])\n",
    "        # Make every word lowercase \n",
    "        new_comments[i] = new_comments[i].lower()\n",
    "    \n",
    "    words = []\n",
    "    for comment in new_comments:\n",
    "        tokenized = word_tokenize(comment)\n",
    "        # Remove stopwords\n",
    "        useful_words = [word for word in tokenized if word not in stopwords.words('english')]\n",
    "        # Create dictionary\n",
    "        my_dict = dict([(word, True) for word in useful_words])\n",
    "        # Append to words\n",
    "        words.append(my_dict)\n",
    "    # Return final list\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = format_words(lebron)\n",
    "pos = 0\n",
    "neg = 0\n",
    "for tweet in input_data:\n",
    "    result = classifier.classify(tweet)\n",
    "    if result == 'negative':\n",
    "        neg = neg + 1\n",
    "    if result == 'positive':\n",
    "        pos = pos + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "About 59.82% of comments regarding LeBron's move were negative.\n"
     ]
    }
   ],
   "source": [
    "print('About {}% of comments regarding LeBron\\'s move were negative.'.format(round((float(neg)/(neg+pos)*100), 2)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
